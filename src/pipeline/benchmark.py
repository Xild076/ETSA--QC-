"""Benchmark utilities for evaluating the sentiment pipeline."""

from __future__ import annotations

import csv
import json
import logging
import math
import random
import re
import xml.etree.ElementTree as ET
import traceback
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, TYPE_CHECKING, Tuple

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns
from networkx.readwrite import json_graph
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
)
from tqdm import tqdm
_LOW_INFORMATION_TOKENS = {
    "i", "me", "my", "mine", "you", "your", "yours", "we", "our", "ours", "they",
    "them", "their", "theirs", "he", "him", "his", "she", "her", "hers", "it", "its",
    "this", "that", "these", "those"
}

try:
    from .pipeline import SentimentPipeline, build_default_pipeline
    from . import config
    from .utility import normalize_text as normalize_text_for_comparison, STOPWORDS
except ImportError:
    from pipeline import SentimentPipeline, build_default_pipeline
    import config
    from utility import normalize_text as normalize_text_for_comparison, STOPWORDS


logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from src.pipeline.graph import RelationGraph

SPACE_RE = re.compile(r"\s+")
PUNCT_RE = re.compile(r"[^a-z0-9\s]")


@dataclass
class AspectAnnotation:
    term: str
    polarity: str
    start: int
    end: int


@dataclass
class DatasetItem:
    sentence_id: str
    text: str
    aspects: List[AspectAnnotation]


@dataclass
class PredictedAspect:
    entity_id: int
    canonical: str
    polarity: str
    score: float
    mentions: List[Dict[str, Any]]
    norm: str
    tokens: Tuple[str, ...]
    head: str


@dataclass
class GoldAspect:
    annotation: AspectAnnotation
    norm: str
    tokens: Tuple[str, ...]
    head: str


def _safe_filename(*segments: str) -> str:
    """Generate a filesystem-safe filename from arbitrary segments."""
    names: List[str] = []
    for segment in segments:
        cleaned = re.sub(r"[^0-9a-zA-Z_-]+", "_", segment or "").strip("_")
        if cleaned:
            names.append(cleaned)
    return "_".join(names) if names else "report"


def _graph_snapshot(graph: Optional["RelationGraph"]) -> Dict[str, Any]:
    """Convert a graph object into a JSON-serialisable snapshot."""
    if graph is None:
        return {}
    try:
        data = json_graph.node_link_data(graph.graph)
        data["clauses"] = list(getattr(graph, "clauses", []))
        data["text"] = getattr(graph, "text", "")
        data["aggregate_sentiments"] = dict(getattr(graph, "aggregate_sentiments", {}))
        return data
    except Exception:
        return {}


def _persist_graph_snapshot(snapshot: Dict[str, Any], directory: Path, sequence: int, sentence_id: str, issue_type: str) -> Optional[Path]:
    """Persist a previously captured graph snapshot to disk."""
    if not snapshot:
        return None
    filename = f"{sequence:05d}_{_safe_filename(sentence_id, issue_type)}.json"
    path = directory / filename
    with path.open("w", encoding="utf-8") as handle:
        json.dump(snapshot, handle, indent=2)
    return path


def _module_hypothesis(issue_type: str) -> List[str]:
    """Return likely pipeline components responsible for an issue type."""
    mapping = {
        "missing_aspect": ["aspect_extractor", "relation_extractor"],
        "spurious_aspect": ["relation_extractor", "modifier_extractor"],
        "wrong_polarity": ["sentiment_analysis", "aggregate_sentiment_model"],
        "pipeline_exception": ["pipeline"],
    }
    return mapping.get(issue_type, ["unknown"])


def _predicted_to_dict(pred: PredictedAspect) -> Dict[str, Any]:
    return {
        "entity_id": pred.entity_id,
        "canonical": pred.canonical,
        "polarity": pred.polarity,
        "score": pred.score,
        "mentions": pred.mentions,
        "norm": pred.norm,
        "tokens": list(pred.tokens),
        "head": pred.head,
    }


def _gold_to_dict(gold: GoldAspect) -> Dict[str, Any]:
    return {
        "term": gold.annotation.term,
        "polarity": gold.annotation.polarity,
        "start": gold.annotation.start,
        "end": gold.annotation.end,
        "norm": gold.norm,
        "tokens": list(gold.tokens),
        "head": gold.head,
    }


def _closest_pred_for_gold(gold: GoldAspect, predicted: Sequence[PredictedAspect]) -> Tuple[Optional[PredictedAspect], float]:
    best: Optional[PredictedAspect] = None
    best_score = 0.0
    for pred in predicted:
        score = _lenient_similarity(pred, gold)
        if score > best_score:
            best_score = score
            best = pred
    return best, best_score


def _closest_gold_for_pred(pred: PredictedAspect, golds: Sequence[GoldAspect]) -> Tuple[Optional[GoldAspect], float]:
    best: Optional[GoldAspect] = None
    best_score = 0.0
    for gold in golds:
        score = _lenient_similarity(pred, gold)
        if score > best_score:
            best_score = score
            best = gold
    return best, best_score


def _normalize_text(text: str) -> str:
    lowered = text.lower()
    cleaned = PUNCT_RE.sub(" ", lowered)
    collapsed = SPACE_RE.sub(" ", cleaned).strip()
    return collapsed

def _is_low_information(token: str) -> bool:
    token_norm = token.strip().lower()
    return not token_norm or len(token_norm) <= 1 or token_norm in _LOW_INFORMATION_TOKENS


def _tokens(text: str) -> Tuple[str, ...]:
    norm = _normalize_text(text)
    tokens = [tok for tok in norm.split(" ") if tok and tok not in STOPWORDS]
    return tuple(tokens)


def _head(tokens: Sequence[str], fallback: str) -> str:
    if tokens:
        return tokens[-1]
    norm = _normalize_text(fallback)
    return norm.split(" ")[-1] if norm else ""


def _lenient_similarity(pred: PredictedAspect, gold: GoldAspect) -> float:
    pred_norm = pred.norm
    gold_norm = gold.norm
    if not pred_norm or not gold_norm:
        return 0.0
    if pred_norm == gold_norm:
        return 1.0
    
    score = 0.0
    pred_set = set(pred.tokens)
    modifier_tokens: set[str] = set()
    for mention in getattr(pred, "mentions", []) or []:
        for modifier in mention.get("modifiers", []) or []:
            modifier_tokens.update(_tokens(modifier))
    pred_pool = pred_set | modifier_tokens
    gold_set = set(gold.tokens)
    min_substring_len = 3

    pred_low_info = True
    gold_low_info = True
    if pred_pool:
        pred_low_info = all(_is_low_information(tok) for tok in pred_pool)
    else:
        pred_low_info = _is_low_information(pred.head) if pred.head else True
    if gold_set:
        gold_low_info = all(_is_low_information(tok) for tok in gold_set)
    else:
        gold_low_info = _is_low_information(gold.head) if gold.head else True

    if pred_pool and gold_set:
        # Exact head word match - strong signal
        if pred.head and gold.head and pred.head == gold.head:
            score = max(score, 0.85)

        # Subset matching (e.g., "screen" vs "lcd screen")
        if pred_pool <= gold_set or gold_set <= pred_pool:
            overlap = min(len(pred_pool), len(gold_set)) / max(len(pred_pool), len(gold_set))
            score = max(score, 0.8 + 0.2 * overlap)
        
        # Jaccard similarity for general overlap
        common = pred_pool & gold_set
        if common:
            jaccard = len(common) / max(len(pred_pool | gold_set), 1)
            score = max(score, 0.7 + 0.3 * jaccard)
        
        # Compound term semantic matching for cases like "Build Quality" vs "construction"
        if len(gold_set) > 1 and not common and not pred_low_info:
            substring_matched = False
            if len(pred_norm) >= min_substring_len:
                for gold_token in gold_set:
                    if len(gold_token) < min_substring_len:
                        continue
                    if gold_token in pred_norm or pred_norm in gold_token:
                        score = max(score, 0.55)
                        substring_matched = True
                        break

            if not substring_matched:
                compound_indicators = {
                    'quality': {'construction', 'design', 'build', 'material', 'finish'},
                    'build': {'construction', 'design', 'material', 'assembly'},
                    'os': {'system', 'software', 'interface', 'platform'},
                    'screen': {'display', 'monitor', 'panel', 'lcd'},
                    'battery': {'power', 'charge', 'life'},
                    'keyboard': {'keys', 'typing', 'input'},
                    'performance': {'speed', 'fast', 'slow', 'quick'},
                    'price': {'cost', 'expensive', 'cheap', 'value'},
                }

                for gold_token in gold_set:
                    related_terms = compound_indicators.get(gold_token, set())
                    if pred.head in related_terms or any(pt in related_terms for pt in pred_pool):
                        score = max(score, 0.60)
                        break
                    if pred.head in compound_indicators:
                        if gold.head in compound_indicators[pred.head]:
                            score = max(score, 0.60)
                            break

    # SequenceMatcher as a fallback
    seq_ratio = SequenceMatcher(None, pred_norm, gold_norm).ratio()
    score = max(score, seq_ratio * 0.9)

    if pred_low_info and not gold_low_info:
        score = min(score, 0.45)
    
    return min(score, 1.0)


def _match_aspects(
    predicted: List[PredictedAspect],
    golds: List[GoldAspect],
    min_score: float = 0.5,
) -> Tuple[List[Tuple[GoldAspect, PredictedAspect, float]], List[GoldAspect], List[PredictedAspect]]:
    """Greedy match between predicted and gold aspects using similarity scores."""
    if not predicted or not golds:
        return [], list(golds), list(predicted)

    scores: List[Tuple[int, int, float]] = []
    for gi, g in enumerate(golds):
        for pi, p in enumerate(predicted):
            sim = _lenient_similarity(p, g)
            if sim >= min_score:
                scores.append((gi, pi, sim))

    used_gold = set()
    used_pred = set()
    matches: List[Tuple[GoldAspect, PredictedAspect, float]] = []
    for gi, pi, sim in sorted(scores, key=lambda x: x[2], reverse=True):
        if gi in used_gold or pi in used_pred:
            continue
        used_gold.add(gi)
        used_pred.add(pi)
        matches.append((golds[gi], predicted[pi], sim))

    unmatched_gold = [g for idx, g in enumerate(golds) if idx not in used_gold]
    unmatched_pred = [p for idx, p in enumerate(predicted) if idx not in used_pred]
    return matches, unmatched_gold, unmatched_pred


def _load_semeval_xml(path: Path) -> List[DatasetItem]:
    """Parse a SemEval XML file into dataset items."""
    tree = ET.parse(path)
    root = tree.getroot()
    dataset: List[DatasetItem] = []
    for sentence_el in root.iterfind("sentence"):
        sid = sentence_el.get("id", "")
        text_el = sentence_el.find("text")
        text = text_el.text.strip() if text_el is not None and text_el.text else ""
        aspects: List[AspectAnnotation] = []
        aspects_el = sentence_el.find("aspectTerms")
        if aspects_el is not None:
            for term_el in aspects_el.findall("aspectTerm"):
                term = term_el.get("term", "").strip()
                if not term or term.lower() == "null":
                    continue
                polarity = term_el.get("polarity", "neutral").lower()
                start = int(term_el.get("from", 0))
                end = int(term_el.get("to", 0))
                aspects.append(AspectAnnotation(term=term, polarity=polarity, start=start, end=end))
        dataset.append(DatasetItem(sentence_id=sid, text=text, aspects=aspects))
    return dataset

def get_dataset(dataset_name: str) -> List[DatasetItem]:
    """Load and shuffle the requested benchmark dataset."""
    import random
    path_map = {
        "test_restaurant_2014": config.SEMEVAL_2014_RESTAURANT_TEST,
        "test_laptop_2014": config.SEMEVAL_2014_LAPTOP_TEST,
    }
    path = path_map.get(dataset_name)
    if not path or not path.exists():
        raise FileNotFoundError(f"Dataset '{dataset_name}' not found at expected path: {path}")
    dataset = _load_semeval_xml(path)
    random.shuffle(dataset)
    return dataset


def _prepare_gold(aspects: Iterable[AspectAnnotation]) -> List[GoldAspect]:
    """Normalise gold annotations for downstream similarity comparisons."""
    prepared: List[GoldAspect] = []
    for asp in aspects:
        tokens = _tokens(asp.term)
        prepared.append(
            GoldAspect(
                annotation=asp,
                norm=_normalize_text(asp.term),
                tokens=tokens,
                head=_head(tokens, asp.term),
            )
        )
    return prepared


def _prepare_predicted(
    aggregate_results: Dict[Any, Dict[str, Any]],
    pos_thresh: float,
    neg_thresh: float,
    *,
    fallback_text: Optional[str] = None,
    graph: Optional["RelationGraph"] = None,
) -> List[PredictedAspect]:
    """Transform aggregate graph results into comparable predicted aspects."""
    predicted: List[PredictedAspect] = []
    for entity_id, data in aggregate_results.items():
        canonical = data.get("label", f"entity_{entity_id}")
        score = float(data.get("aggregate_sentiment", 0.0) or 0.0)
        
                               
        if math.isnan(score) or math.isinf(score): 
            score = 0.0
                                          
        score = max(-1.0, min(1.0, score))
        
        polarity = "neutral"
        if score >= pos_thresh: 
            polarity = "positive"
        elif score <= neg_thresh: 
            polarity = "negative"
        
        tokens = _tokens(canonical)
        predicted.append(
            PredictedAspect(
                entity_id=int(entity_id), canonical=canonical, polarity=polarity, score=score,
                mentions=data.get("mentions", []), norm=_normalize_text(canonical),
                tokens=tokens, head=_head(tokens, canonical),
            )
        )
    
    if predicted or not fallback_text or not fallback_text.strip():
        return predicted

                                                      
    score = 0.0
    if graph is not None and hasattr(graph, "compute_text_sentiment"):
        try:
            score = float(graph.compute_text_sentiment(fallback_text))
        except Exception:
            pass                    
    
    if math.isnan(score) or math.isinf(score): 
        score = 0.0
    score = max(-1.0, min(1.0, score))
    
    polarity = "neutral"
    if score >= pos_thresh: 
        polarity = "positive"
    elif score <= neg_thresh: 
        polarity = "negative"
    
    tokens = _tokens(fallback_text)
    predicted.append(
        PredictedAspect(
            entity_id=0, canonical=fallback_text.strip(), polarity=polarity, score=score,
            mentions=[{"text": fallback_text.strip(), "clause_index": 0}],
            norm=_normalize_text(fallback_text), tokens=tokens, head=_head(tokens, fallback_text),
        )
    )
    return predicted


def _normalize_gold_polarity(polarity: str) -> str:
    """Normalize gold polarity labels to a known set."""
    value = (polarity or "").lower().strip()
    return value if value in {"positive", "negative", "neutral", "conflict"} else "neutral"


def _classification_labels(gold_labels: List[str], pred_labels: List[str]) -> List[str]:
    """Combine gold and predicted labels into a sorted list for reporting."""
    return sorted(set(gold_labels) | set(pred_labels)) or ["neutral"]


def _serialize_report(report: Dict[str, Any]) -> Dict[str, Any]:
    """Convert sklearn reports to JSON-serialisable dictionaries."""
    serializable = {}
    for label, metrics in report.items():
        if isinstance(metrics, dict):
            serializable[label] = {k: int(v) if k == 'support' else float(v) for k, v in metrics.items()}
        else:
            serializable[label] = float(metrics)
    return serializable


def _plot_confusion_matrix(cm: List[List[int]], labels: List[str], path: Path) -> None:
    """Create and persist a confusion matrix plot."""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", xticklabels=labels, yticklabels=labels, cmap="Blues")
    plt.ylabel("Gold")
    plt.xlabel("Predicted")
    plt.tight_layout()
    plt.savefig(path)
    plt.close()


def run_benchmark(
    run_name: str,
    dataset_name: str,
    run_mode: str = "full_stack",
    limit: Optional[int] = None,
    pos_thresh: float = 0.1,
    neg_thresh: float = -0.1,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    shuffle_seed: Optional[int] = None,
    ablate_modifiers: bool = False,
    ablate_relations: bool = False,
) -> Dict[str, Any]:
    """Execute the full benchmark pipeline and return aggregated metrics."""
    
    items = get_dataset(dataset_name)

    if shuffle_seed is None:
        shuffle_seed = int(datetime.now().timestamp() * 1_000_000)
    rng = random.Random(shuffle_seed)
    rng.shuffle(items)

    if limit and limit > 0:
        items = items[:limit]
    
    total_items = len(items)
    logger.info(
        f"Starting benchmark: dataset={dataset_name}, run_mode={run_mode}, samples={total_items}, shuffle_seed={shuffle_seed}"
    )

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_run_name = f"{run_name}_{dataset_name}_{timestamp}"
    run_dir = config.BENCHMARK_OUTPUT_DIR / final_run_name
    run_dir.mkdir(parents=True, exist_ok=True)
    
    graph_reports_dir = run_dir / "graph_reports"
    graph_reports_subdirs = {
        "spurious_aspect": graph_reports_dir / "spurious_aspect",
        "wrong_polarity": graph_reports_dir / "wrong_polarity",
        "default": graph_reports_dir / "other",
    }
    for p in graph_reports_subdirs.values():
        p.mkdir(parents=True, exist_ok=True)

    log_path = run_dir / "benchmark.log"
    file_handler = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)

    # Build pipeline with optional ablation modes
    if ablate_modifiers or ablate_relations:
        logger.info("Building pipeline with ablation mode: modifiers=%s, relations=%s", ablate_modifiers, ablate_relations)
    
    pipeline = build_default_pipeline(
        ablate_modifiers=ablate_modifiers,
        ablate_relations=ablate_relations
    )

    gold_labels, pred_labels, error_details, error_rows, match_rows, sentence_records = [], [], [], [], [], []
    error_summary = Counter()
    sentence_precision_values, sentence_recall_values, sentence_f1_values = [], [], []
    stats = defaultdict(int)
    stats["requested_sentences"] = total_items
    graph_sequence = 1

    def _register_error(
        issue_type: str, item: DatasetItem, snapshot: Dict[str, Any],
        gold: Optional[GoldAspect], pred: Optional[PredictedAspect],
        match_score: Optional[float], analysis: Dict[str, Any],
    ) -> None:
        nonlocal graph_sequence
        target_dir = graph_reports_subdirs.get(issue_type, graph_reports_subdirs["default"])
        path = _persist_graph_snapshot(snapshot, target_dir, graph_sequence, item.sentence_id, issue_type)
        graph_ref = str(path.relative_to(config.PROJECT_ROOT)) if path else None
        if path: graph_sequence += 1
        
        modules = _module_hypothesis(issue_type)
        error_details.append({
            "sentence_id": item.sentence_id, "text": item.text, "issue_type": issue_type,
            "probable_modules": modules, "graph_report": graph_ref,
            "gold": _gold_to_dict(gold) if gold else None,
            "predicted": _predicted_to_dict(pred) if pred else None,
            "match_score": match_score, "analysis": analysis,
        })
        error_summary[issue_type] += 1
        error_rows.append({
            "issue_type": issue_type, "probable_modules": ";".join(modules), "graph_report": graph_ref or "",
            "id": item.sentence_id, "text": item.text,
            "gold_aspect": gold.annotation.term if gold else analysis.get("closest_gold_term", ""),
            "gold_polarity": _normalize_gold_polarity(gold.annotation.polarity) if gold else analysis.get("closest_gold_polarity", ""),
            "pred_aspect": pred.canonical if pred else analysis.get("closest_pred_aspect", ""),
            "pred_polarity": pred.polarity if pred else analysis.get("closest_pred_polarity", ""),
            "pred_score": pred.score if pred else analysis.get("closest_pred_score", ""),
            "match_score": match_score if match_score is not None else analysis.get("similarity_score", ""),
            "analysis": json.dumps(analysis, sort_keys=True),
        })
        logger.warning(f"Detected {issue_type} on sentence {item.sentence_id}")

    try:
        with tqdm(items, total=total_items, desc="Benchmark", unit="sentence") as progress:
            for i, item in enumerate(progress):
                # Skip if no gold aspects to save time
                gold_aspects = _prepare_gold(item.aspects)
                if len(gold_aspects) == 0:
                    stats["skipped_no_gold"] = stats.get("skipped_no_gold", 0) + 1
                    continue
                
                has_error, sentence_issue_types = False, []
                try:
                    result = pipeline.process(item.text, debug=False)
                except Exception as exc:
                    stats["processing_errors"] += 1; stats["sentence_errors"] += 1
                    logger.exception(f"Pipeline failed on sentence {item.sentence_id}")
                    analysis = {"exception": repr(exc), "traceback": traceback.format_exc()}
                    _register_error("pipeline_exception", item, {}, None, None, None, analysis)
                    sentence_records.append({ "sentence_id": item.sentence_id, "text": item.text, "issue_types": ["pipeline_exception"], "precision": 0.0, "recall": 0.0, "f1": 0.0 })
                    sentence_precision_values.append(0.0); sentence_recall_values.append(0.0); sentence_f1_values.append(0.0)
                    continue

                aggregate_results = result.get("aggregate_results") or {}
                predicted_aspects = _prepare_predicted(
                    aggregate_results, pos_thresh, neg_thresh,
                    fallback_text=item.text, graph=result.get("graph"),
                )
                graph_snapshot = _graph_snapshot(result.get("graph"))

                matches, unmatched_gold, unmatched_pred = _match_aspects(predicted_aspects, gold_aspects)
                
                # Fallback: if no matches, use overall sentence sentiment for ALL gold aspects
                if len(matches) == 0 and len(gold_aspects) > 0:
                    stats["fallback_applied"] = stats.get("fallback_applied", 0) + 1
                    
                    fallback_score = 0.0
                    graph_obj = result.get("graph")
                    if graph_obj is not None and hasattr(graph_obj, "compute_text_sentiment"):
                        try:
                            fallback_score = float(graph_obj.compute_text_sentiment(item.text))
                        except Exception:
                            pass
                    if math.isnan(fallback_score) or math.isinf(fallback_score):
                        fallback_score = 0.0
                    fallback_score = max(-1.0, min(1.0, fallback_score))
                    
                    fallback_polarity = "neutral"
                    if fallback_score >= pos_thresh:
                        fallback_polarity = "positive"
                    elif fallback_score <= neg_thresh:
                        fallback_polarity = "negative"
                    
                    logger.warning(
                        f"FALLBACK APPLIED: overall sentiment {fallback_polarity} ({fallback_score:+.3f}) "
                        f"to {len(gold_aspects)} gold aspects {[g.annotation.term for g in gold_aspects]} in \"{item.text}\""
                    )
                    
                    # Create a predicted aspect for each gold aspect using overall sentiment
                    predicted_aspects = []
                    for gold_aspect in gold_aspects:
                        gold_tokens = _tokens(gold_aspect.annotation.term)
                        predicted_aspects.append(PredictedAspect(
                            entity_id=0, canonical=gold_aspect.annotation.term, polarity=fallback_polarity, score=fallback_score,
                            mentions=[{"text": gold_aspect.annotation.term, "clause_index": 0}],
                            norm=_normalize_text(gold_aspect.annotation.term), tokens=gold_tokens, head=_head(gold_tokens, gold_aspect.annotation.term),
                        ))
                    
                    # Re-match with fallback predictions
                    matches, unmatched_gold, unmatched_pred = _match_aspects(predicted_aspects, gold_aspects)
                
                stats["total_sentences"] += 1
                stats.update({
                    "gold_aspects": stats["gold_aspects"] + len(gold_aspects),
                    "pred_aspects": stats["pred_aspects"] + len(predicted_aspects),
                    "matched_aspects": stats["matched_aspects"] + len(matches),
                    "unmatched_gold": stats["unmatched_gold"] + len(unmatched_gold),
                    "unmatched_pred": stats["unmatched_pred"] + len(unmatched_pred),
                })

                gold_count, pred_count, match_count = len(gold_aspects), len(predicted_aspects), len(matches)
                prec = match_count / pred_count if pred_count else (1.0 if gold_count == 0 else 0.0)
                rec = match_count / gold_count if gold_count else (1.0 if pred_count == 0 else 0.0)
                f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0
                sentence_precision_values.append(prec); sentence_recall_values.append(rec); sentence_f1_values.append(f1)

                if match_count == 0 and (gold_count or pred_count):
                    gold_terms = [g.annotation.term for g in gold_aspects]
                    pred_terms = [p.canonical for p in predicted_aspects]
                    logger.warning(
                        "Detected zero aspect matches on sentence %s (gold=%s predicted=%s)",
                        item.sentence_id,
                        gold_terms or [],
                        pred_terms or [],
                    )

                for gold, pred, sim in matches:
                    gold_pol, pred_pol = _normalize_gold_polarity(gold.annotation.polarity), pred.polarity
                    gold_labels.append(gold_pol); pred_labels.append(pred_pol)
                    record = { "issue_type": "correct", "id": item.sentence_id, "text": item.text,
                               "gold_aspect": gold.annotation.term, "gold_polarity": gold_pol,
                               "pred_aspect": pred.canonical, "pred_polarity": pred_pol,
                               "pred_score": pred.score, "match_score": sim }
                    if gold_pol != pred_pol and gold_pol != 'conflict':
                        has_error = True; sentence_issue_types.append("wrong_polarity")
                        record["issue_type"] = "wrong_polarity"
                        analysis = {"predicted_score": pred.score, "gold_polarity": gold_pol, "predicted_polarity": pred_pol}
                        logger.warning(
                            f"wrong_polarity: '{gold.annotation.term}' - gold={gold_pol} pred={pred_pol} ({pred.score:+.3f}) in: \"{item.text}\""
                        )
                        _register_error("wrong_polarity", item, graph_snapshot, gold, pred, sim, analysis)
                    match_rows.append(record)

                for gold in unmatched_gold:
                    has_error = True; sentence_issue_types.append("missing_aspect")
                    closest_pred, closest_score = _closest_pred_for_gold(gold, predicted_aspects)
                    analysis = {"detail": "gold aspect not matched", "similarity_score": closest_score, "closest_pred": _predicted_to_dict(closest_pred) if closest_pred else None}
                    _register_error("missing_aspect", item, graph_snapshot, gold, closest_pred, None, analysis)
                
                for pred in unmatched_pred:
                    has_error = True; sentence_issue_types.append("spurious_aspect")
                    closest_gold, closest_score = _closest_gold_for_pred(pred, gold_aspects)
                    analysis = {"detail": "predicted aspect without gold match", "similarity_score": closest_score, "closest_gold": _gold_to_dict(closest_gold) if closest_gold else None}
                    _register_error("spurious_aspect", item, graph_snapshot, closest_gold, pred, None, analysis)

                sentence_records.append({ "sentence_id": item.sentence_id, "text": item.text, "gold_count": gold_count,
                                           "pred_count": pred_count, "matched_count": match_count, "issue_types": sorted(set(sentence_issue_types)),
                                           "precision": prec, "recall": rec, "f1": f1 })
                if has_error:
                    stats["sentence_errors"] += 1
                
                progress.set_postfix(errors=stats["sentence_errors"], acc=f"{accuracy_score(gold_labels, pred_labels):.3f}" if gold_labels else 0.0)

    finally:
        logger.removeHandler(file_handler)
        file_handler.close()

    labels = _classification_labels(gold_labels, pred_labels)
    cm = confusion_matrix(gold_labels, pred_labels, labels=labels).tolist() if gold_labels else []
    report = classification_report(gold_labels, pred_labels, labels=labels, output_dict=True, zero_division=0) if gold_labels else {}
    
    precision = stats["matched_aspects"] / stats["pred_aspects"] if stats["pred_aspects"] else 0.0
    recall = stats["matched_aspects"] / stats["gold_aspects"] if stats["gold_aspects"] else 0.0
    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0

    metrics = {
        "run_name": final_run_name, "dataset": dataset_name, "mode": run_mode,
        "total_sentences": total_items, "processed_sentences": stats["total_sentences"],
        "sentence_error_rate": stats["sentence_errors"] / total_items if total_items else 0.0,
        "aspect_precision": precision, "aspect_recall": recall, "aspect_f1": f1_score,
        "avg_sentence_f1": sum(sentence_f1_values) / len(sentence_f1_values) if sentence_f1_values else 0.0,
        "accuracy": accuracy_score(gold_labels, pred_labels) if gold_labels else 0.0,
        "balanced_accuracy": balanced_accuracy_score(gold_labels, pred_labels, adjusted=True) if gold_labels and len(set(gold_labels)) > 1 else 0.0,
        "classification_report": _serialize_report(report),
        "error_summary": dict(error_summary),
        "confusion_matrix": {"labels": labels, "matrix": cm},
        "graph_reports_dir": str(graph_reports_dir.relative_to(config.PROJECT_ROOT)),
        "shuffle_seed": shuffle_seed,
    }

    with (run_dir / "metrics.json").open("w", encoding="utf-8") as f: json.dump(metrics, f, indent=2)
    if cm: _plot_confusion_matrix(cm, labels, run_dir / "confusion_matrix.png")
    with (run_dir / "error_details.json").open("w", encoding="utf-8") as f: json.dump(error_details, f, indent=2)
    with (run_dir / "sentence_summary.json").open("w", encoding="utf-8") as f: json.dump(sentence_records, f, indent=2)
    
    csv_fields = ["issue_type", "probable_modules", "graph_report", "id", "text", "gold_aspect", "gold_polarity", "pred_aspect", "pred_polarity", "pred_score", "match_score", "analysis"]
    with (run_dir / "error_analysis.csv").open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=csv_fields)
        writer.writeheader()
        if error_rows: writer.writerows(error_rows)

    with (run_dir / "matches.json").open("w", encoding="utf-8") as f: json.dump(match_rows, f, indent=2)

    fallback_count = stats.get("fallback_applied", 0)
    logger.info(
        f"Benchmark complete: F1={f1_score:.3f} P={precision:.3f} R={recall:.3f} Acc={metrics['accuracy']:.3f} "
        f"Total Errors={sum(error_summary.values())} Sentences w/ Errors={stats['sentence_errors']} "
        f"Fallback Applied={fallback_count}"
    )

    return metrics