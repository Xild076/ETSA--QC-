*Prof. Adam Pines reviewed and has the following feedback:*
> This looks like really cool work, congrats on the progress to get to this point.
Main feedback is to get a very strong null model, or potentially multiple null models. I think the hypothesis is probably right, but you'll always be able to explain more with more parameters. I think you want to make sure you are set up to demonstrate something above-and-beyond more parameters = better fit.
It might not be so many parameters that you need regularization, but held-out data would certainly be a good starting point to demonstrate that you're not just blindly fitting the data (I assume you are already dialed into train/test if you're part of a CS group). The harder part will be to think of competing models for nulls. You could imagine permuting the sentences in some way, but that might require more human ratings and it's potentially trivial to demonstrate that real sentences have more semantic impact than permuted ones.
The second piece of making compelling work here that would help with making strong null models is maximal specificity in the exact temporal and relational contexts you are trying to capture. This isn't really my field, but I imagine the model will likely be sensitive to a few underlying psychological phenomena.
Definitely on the right track though. Would be very cool to see the model applied to familiar works of literature if it works out.